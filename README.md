# 분산 백업 시스템

## 시스템 개요

현대의 데이터 처리 환경에서는 대용량 파일을 빠르고 안정적으로 저장하고 복원하는 기술이 필수적입니다. 본 프로젝트는 이러한 필요성을 해결하기 위해 **Rabin Fingerprint 기반의 Content-Defined Chunking (CDC)** 기법과 **분산 네트워크 기반 저장 구조**를 결합한 **병렬 분산 백업 및 복원 시스템**을 구현하였습니다.

이 시스템은 데이터를 고정된 크기로 나누지 않고 데이터의 내용에 따라 동적으로 나누는 방식(Content-defined chunking)을 통해 **중복 제거**와 **전송 효율성**을 높였으며, **분산 노드 간 병렬 전송 및 병렬 복원**을 통해 전체 처리 속도를 극대화하였습니다.

특히, 백업 과정에서는 청크 데이터를 **SHA-256 해시**를 기반으로 식별하며, 각 청크의 메타데이터(offset, 노드 정보 등)는 **JSON 형식의 메타파일**로 관리됩니다. 복원 과정에서는 이 메타데이터를 기반으로 **병렬적으로 청크를 요청하고**, 파일의 적절한 위치에 `pwrite()`를 통해 데이터를 정확하게 복원합니다.

본 프로젝트는 시스템 설계 및 구현 방법, 최적화 전략, 성능 평가 결과를 포함하여 전체적인 아키텍처 및 실험 분석을 다룹니다. 이를 통해 병렬화와 분산 처리가 어떻게 백업/복원 성능을 향상시키는지를 구체적으로 설명합니다.

## 기능 요약

- **Content-Defined Chunking (CDC) 기반 파일 분할**
  - 고정된 블록 크기 대신, Rabin Fingerprint 기반의 슬라이딩 윈도우 해시 알고리즘을 사용하여 데이터 내용에 따라 동적으로 청크 경계를 설정합니다.
  - 이 방식은 동일한 내용이 다르게 배치된 파일에서도 중복을 효과적으로 감지할 수 있어, 저장 공간 절약 및 전송 효율성 향상에 기여합니다.

- **SHA-256 기반 청크 식별자 생성**
  - 각 청크는 SHA-256 해시 알고리즘을 사용하여 해시값으로 식별되며, 이 해시는 청크 데이터의 무결성을 확인하는 데에도 사용됩니다.
  - 해시값 충돌을 방지하면서도 동일한 청크는 중복 저장하지 않도록 설계되었습니다.

- **병렬 백업 (Parallel Store)**
  - OpenMP 기반 멀티스레딩 구조로 구현되어, 하나의 파일을 여러 스레드가 동시에 CDC를 수행하며 청크를 분할합니다.
  - 각 스레드는 분산된 여러 노드에 대해 **전용 TCP Persistent Connection**을 열고, 병렬로 청크를 전송합니다.
  - 이를 통해 각 노드 간 병목을 줄이고, TCP 접속/해제 비용을 줄여 전송 효율을 극대화합니다.

- **병렬 복원 (Parallel Restore)**
  - JSON 메타파일을 기반으로, 여러 스레드가 각자 할당된 청크 정보를 읽고 해당 노드에 병렬로 요청합니다.
  - 복원 시에도 **스레드 수 × 노드 수만큼의 TCP 연결을 병렬로 유지**하여 효율적인 데이터 수신을 보장하며, `pwrite()`를 통해 파일의 정확한 offset 위치에 직접 기록합니다.
  - 전체 파일 복원이 병렬로 진행되기 때문에 대용량 복원 시에도 성능이 뛰어납니다.

- **메타데이터 기반 청크 관리**
  - 각 청크의 고유 ID, 저장 위치(offset), 해당 노드의 IP 및 포트를 JSON 형식으로 저장합니다.
  - 이를 통해 빠르고 정확한 복원뿐만 아니라, 시스템 상태나 데이터 흐름 추적에도 유용합니다.

- **병렬 접속 슬레이브 노드**
  - 병렬 처리 시, 마스터 슬레이브의 스레드는 각 슬레이브 노드에 대해 독립적인 TCP 연결을 유지하여 충돌 없이 안정적으로 데이터를 주고받습니다.
  - Persistent Connection을 유지하며, 여러 요청을 한 연결로 처리할 수 있도록 설계되었습니다.

- **직렬 처리(Serial Store/Restore)도 지원합니다**
  - 병렬 처리와의 성능 차이를 분석할 수 있도록, 직렬 방식의 백업 및 복원 기능 또한 함께 구현되어 있습니다.
  - 단일 스레드로 전체 파일을 처리하여 병렬화의 효과를 정량적으로 비교할 수 있습니다.

- **성능 벤치마크 자동화**
  - Python 기반 스크립트를 통해 다양한 파일 크기에 대한 저장/복원 성능을 자동으로 측정하고, 결과를 CSV로 저장합니다.
  - 저장 시간, 복원 시간, 성공 여부 등 핵심 지표들을 비교하여 시스템 성능을 분석하고 시각화할 수 있습니다.

## 직접 실행 방법

### 1. Docker Compose로 컨테이너를 실행합니다
```bash
docker-compose up --build
```

### 2. 마스터 노드 컨테이너에 접속합니다
```bash
docker exec -it master_node /bin/bash
```

### 3. 데이터를 저장합니다
•	직렬 저장 (Serial Store)
```bash
./store data_1000.bin serial_chunk_map.json 172.28.0.11 9001 172.28.0.12 9001 172.28.0.13 9001
```
•	병렬 저장 (Parallel Store)
```bash
./parrel_store data_1000.bin parrel_chunk_map.json 172.28.0.11 9001 172.28.0.12 9001 172.28.0.13 9001
```

### 4. 데이터를 복원합니다
•	직렬 복원 (Serial Restore)
```bash
./restore serial_chunk_map.json serial_restored_data.bin
```
•	병렬 복원 (Parallel Restore)
```bash
./parrel_restore parrel_chunk_map.json parrel_restored_data.bin
```

### 5. 복원 검증을 수행합니다
```bash
diff data_1000.bin serial_restored_data.bin && echo "Serial Restore successful!" || echo "Serial Restore failed!"
diff data_1000.bin parrel_restored_data.bin && echo "Parallel Restore successful!" || echo "Parallel Restore failed!"
```

## 벤치마크 실행 방법

### 1. Docker Compose로 컨테이너를 실행합니다
```bash
docker-compose up --build
```

### 2. 마스터 노드 컨테이너에 접속합니다
```bash
docker exec -it master_node /bin/bash
```

### 3. 일괄 성능 비교 분석을 수행합니다 (Benchmark 실행)
```bash
python3 store_benchmark.py
python3 restore_benchmark.py
```

### 4. 결과 CSV 파일을 확인하기 위해 로컬로 복사합니다
```bash
docker cp master_node:/usr/src/app/store_benchmark_results.csv ./store_benchmark_results.csv
docker cp master_node:/usr/src/app/restore_benchmark_results.csv ./restore_benchmark_results.csv
```

## 실험 결과

### 컬럼
  •	threads: 사용한 OpenMP 쓰레드 수
  •	file_size_mb: 실험에 사용된 파일 크기 (MB 단위)
  •	serial_time: 직렬(Serial) 백업 수행 시간 (초)
  •	parallel_time: 병렬(Parallel) 백업 수행 시간 (초)
  •	speedup: 속도 향상 배수 (Speedup = Serial Time / Parallel Time)
  •	efficiency: 병렬 효율성 (Efficiency = Speedup / Threads × 100%)

### 분석표
| **threads** | **file_size_mb** | **serial_time**   | **parallel_time** | **speedup**        | **efficiency**     |
| ----------- | ---------------- | ----------------- | ----------------- | ------------------ | ------------------ |
| **2**       | 100              | 0.936583995819092 | 0.643244981765747 | 1.4560300078021800 | 72.80150039010890  |
| **2**       | 500              | 4.05113673210144  | 3.25694489479065  | 1.2438456476746300 | 62.19228238373130  |
| **2**       | 1000             | 8.76207089424133  | 6.54021215438843  | 1.339722731832490  | 66.98613659162450  |
| **2**       | 2000             | 17.0826187133789  | 14.2213265895844  | 1.2011972726855200 | 60.05986363427620  |
| **2**       | 3000             | 24.4595453739166  | 21.0929200649261  | 1.1596092574488400 | 57.98046287244180  |
| **3**       | 100              | 0.816385269165039 | 0.43789529800415  | 1.8643389707219500 | 62.144632357398400 |
| **3**       | 500              | 3.9430410861969   | 2.22487354278564  | 1.772254022698310  | 59.075134089943800 |
| **3**       | 1000             | 9.10470414161682  | 4.94011616706848  | 1.843014178960020  | 61.433805965333900 |
| **3**       | 2000             | 16.4489645957947  | 10.1188759803772  | 1.6255723093842600 | 54.18574364614210  |
| **3**       | 3000             | 25.1143796443939  | 14.625292301178   | 1.7171882193678300 | 57.23960731226100  |
| **4**       | 100              | 0.76629376411438  | 0.337738752365112 | 2.268894992796030  | 56.72237481990070  |
| **4**       | 500              | 3.6574010848999   | 1.92000603675842  | 1.9048904091336900 | 47.622260228342200 |
| **4**       | 1000             | 9.03192853927612  | 3.53247666358948  | 2.5568261023127200 | 63.92065255781790  |
| **4**       | 2000             | 17.0783772468567  | 7.62405920028687  | 2.240063566953170  | 56.001589173829100 |
| **4**       | 3000             | 25.0755164623261  | 11.9713580608368  | 2.0946258841224000 | 52.365647103060000 |
| **5**       | 100              | 0.777571439743042 | 0.310983419418335 | 2.50036301355685   | 50.007260271137    |
| **5**       | 500              | 4.05235123634338  | 1.68579387664795  | 2.403823677661660  | 48.07647355323320  |
| **5**       | 1000             | 8.98410415649414  | 3.61620926856995  | 2.4843982992297800 | 49.68796598459560  |
| **5**       | 2000             | 17.2647128105164  | 7.11950445175171  | 2.4249879928466800 | 48.49975985693370  |
| **5**       | 3000             | 24.7126650810242  | 10.1617751121521  | 2.431924029835220  | 48.63848059670440  |
| **6**       | 100              | 0.870465517044067 | 0.382256746292114 | 2.277175028269800  | 37.95291713783000  |
| **6**       | 500              | 4.17793202400208  | 1.76255130767822  | 2.370388882185560  | 39.5064813697594   |
| **6**       | 1000             | 8.96087431907654  | 3.65140223503113  | 2.454091262010780  | 40.90152103351310  |
| **6**       | 2000             | 17.1435039043427  | 7.03225660324097  | 2.4378382177410500 | 40.63063696235090  |
| **6**       | 3000             | 25.2005915641785  | 10.7176699638367  | 2.35131251934513   | 39.1885419890855   |
| **7**       | 100              | 0.778440237045288 | 0.293638467788696 | 2.6510158662367700 | 37.871655231953900 |
| **7**       | 500              | 3.80688047409058  | 1.49964952468872  | 2.538513440252490  | 36.264477717892800 |
| **7**       | 1000             | 8.50499725341797  | 3.75754570960999  | 2.2634447883538200 | 32.334925547911600 |
| **7**       | 2000             | 17.1631708145142  | 6.96229124069214  | 2.465161284003970  | 35.216589771485200 |
| **7**       | 3000             | 24.891131401062   | 10.639050245285   | 2.339600887973360  | 33.42286982819090  |
| **8**       | 100              | 0.798559904098511 | 0.276022911071777 | 2.8930928269604900 | 36.16366033700610  |
| **8**       | 500              | 4.22017002105713  | 1.8743052482605   | 2.2515916364070200 | 28.14489545508780  |
| **8**       | 1000             | 8.72153186798096  | 3.64279341697693  | 2.3941878854109600 | 29.927348567637000 |
| **8**       | 2000             | 17.6584265232086  | 6.9709837436676   | 2.5331326499289900 | 31.66415812411230  |
| **8**       | 3000             | 25.7291443347931  | 10.8920810222626  | 2.362188114668320  | 29.527351433354000 |

### 결과 분석

![alt text](<Efficiency for 1000KB File.png>)

스레드 수 증가에 따른 속도 향상(Speedup) 은 4개까지 비교적 꾸준히 증가하며, 이 시점에서 가장 효율이 높습니다.
5스레드 이상부터 효율(Efficiency) 이 급격히 떨어지기 시작합니다. 이는 시스템 자원 경합, 네트워크 대역폭 한계, TCP 연결 수 증가에 따른 오버헤드 등의 복합적인 원인이 있겠지만,
주로 슬레이브 노드는 각 마스터의 쓰레드 수만큼의 TCP 세션을 처리하고 파일 시스템에 저장하는 구조를 가지고 있기 떄문에, 실제 사용되는 쓰레드가 4개가 아닌 16개의 쓰레드가 생성되기 때문이다.

마스터 노드가 OpenMP를 통해 병렬 전송을 수행하고, 각 스레드는 슬레이브 노드와 전용 TCP Persistent Connection을 유지합니다.
슬레이브 노드도 각 마스터 스레드마다 소켓 처리를 위한 스레드를 만들어야 하므로, 전체 시스템의 스레드 수가 급격히 증가하게 됩니다.
이로 인해, 8코어 시스템에서는 일정 스레드 수를 넘어서면 성능 향상이 정체되거나 오히려 떨어지는 현상이 발생합니다.

#### Speedup이 완벽하게 선형적으로 증가하지 않고 이에 따라, Efficiency가 4개의 쓰레드 이후에 감소하는 이유
실제 측정된 speedup은 이론적인 N배까지는 도달하지 못함.
예: 8개의 쓰레드를 사용했을 때도 speedup이 2~4 사이에 머무는 경우가 많음. 이는 다음의 요인 때문.

1. 병렬 오버헤드
각 쓰레드는 자체적으로 Rabin Fingerprint 계산, SHA 해시 계산, JSON 메타데이터 기록, TCP 송수신 등을 수행하며 이 과정에서 오버헤드 발생.
또한 병렬 영역 진입 시 OpenMP의 내부 스케줄링 비용이 발생함.
2. 네트워크 병목 및 슬레이브 노드 처리 병렬성 한계
슬레이브 노드는 각 마스터의 쓰레드 수만큼의 TCP 세션을 처리함.
예: 쓰레드 수 8 → 슬레이브 1개가 동시에 8개 연결 처리 필요.
3. 자원 경합
마스터와 슬레이브가 같은 물리 머신에서 동작하고 있고, 이 머신의 코어 수가 8개로 제한되어 있음.
마스터에서 8개 쓰레드를 돌리고, 동시에 3개의 슬레이브가 각자 여러 연결을 처리 → CPU 및 메모리, 네트워크 인터페이스 경합 증가
이로 인해 병렬 성능이 포화에 도달하거나 심지어 성능 저하가 발생할 수 있음.

특히, 자원 경합의 문제가 4 ~ 5개의 쓰레드를 넘어가면 심각하게 발생하는 것을 알 수 있음.




## 참고 문헌
- A Low-bandwidth Network File System (LBFS)
https://pdos.csail.mit.edu/papers/lbfs:sosp01/lbfs.pdf
- A Low-bandwidth Network File System (LBFS) 번역본
https://hs2g5.tistory.com/entry/A-Low-bandwidth-Network-File-System-%E2%80%93-MIT-LAB-for-Computing-science

- USENIX 논문
https://www.usenix.org/legacy/event/fast11/tech/full_papers/Meyer.pdf
https://www.usenix.org/system/files/conference/atc16/atc16-paper-xia.pdf
https://www.cs.cmu.edu/~15-749/READINGS/required/cas/tridgell96.pdf

- 기술 자료
•	Rabin Fingerprint: https://en.wikipedia.org/wiki/Rabin_fingerprint
•	Chunking & Deduplication Diagram: https://www.researchgate.net/figure/Chunking-and-Deduplication-process_fig1_338252900
•	MS Data Deduplication: https://learn.microsoft.com/en-us/windows-server/storage/data-deduplication/overview
